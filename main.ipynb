{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files\n",
    "train_csv_file = 'labels/classes_train.csv'\n",
    "val_csv_file = 'labels/classes_valid.csv'\n",
    "train_df = pd.read_csv(train_csv_file)\n",
    "val_df = pd.read_csv(val_csv_file)\n",
    "\n",
    "# Define parameters\n",
    "train_image_dir = 'train/'\n",
    "val_image_dir = 'valid/'\n",
    "image_size = (224, 224)\n",
    "num_classes = train_df.shape[1] - 1  # The number of classes (columns - 1 for filename)\n",
    "\n",
    "def preprocess_image(directory, filename):\n",
    "    img_path = os.path.join(directory, filename)\n",
    "    img = load_img(img_path, target_size=image_size)\n",
    "    img = img_to_array(img)\n",
    "    img = img / 255.0  # Normalize to [0, 1]\n",
    "    return img\n",
    "\n",
    "def preprocess_label(row):\n",
    "    return row[1:].values  # All columns except the filename\n",
    "\n",
    "# Create lists for images and labels\n",
    "train_images = []\n",
    "train_labels = []\n",
    "val_images = []\n",
    "val_labels = []\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    filename = row['filename']\n",
    "    label = preprocess_label(row)\n",
    "    \n",
    "    if os.path.exists(os.path.join(train_image_dir, filename)):\n",
    "        image = preprocess_image(train_image_dir, filename)\n",
    "        train_images.append(image)\n",
    "        train_labels.append(label)\n",
    "\n",
    "for index, row in val_df.iterrows():\n",
    "    filename = row['filename']\n",
    "    label = preprocess_label(row)\n",
    "    \n",
    "    if os.path.exists(os.path.join(val_image_dir, filename)):\n",
    "        image = preprocess_image(val_image_dir, filename)\n",
    "        val_images.append(image)\n",
    "        val_labels.append(label)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "val_images = np.array(val_images)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.asarray(train_images, dtype=np.float32)\n",
    "train_labels = np.asarray(train_labels, dtype=np.int64)\n",
    "\n",
    "val_images = np.asarray(val_images, dtype=np.float32)\n",
    "val_labels = np.asarray(val_labels, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create a TensorFlow dataset for training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train_images))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Create a TensorFlow dataset for validation\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained VGG19 model + higher level layers\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom layers on top of VGG19\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(5, activation='sigmoid')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the layers of VGG19\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 513ms/step - accuracy: 0.3809 - loss: 0.4921 - val_accuracy: 0.4646 - val_loss: 0.4455\n",
      "Epoch 2/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 498ms/step - accuracy: 0.5657 - loss: 0.3816 - val_accuracy: 0.5354 - val_loss: 0.4013\n",
      "Epoch 3/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 500ms/step - accuracy: 0.6335 - loss: 0.3360 - val_accuracy: 0.5556 - val_loss: 0.3958\n",
      "Epoch 4/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 500ms/step - accuracy: 0.6256 - loss: 0.3302 - val_accuracy: 0.5859 - val_loss: 0.3730\n",
      "Epoch 5/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 499ms/step - accuracy: 0.6845 - loss: 0.2950 - val_accuracy: 0.6061 - val_loss: 0.3467\n",
      "Epoch 6/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 506ms/step - accuracy: 0.7139 - loss: 0.2825 - val_accuracy: 0.5253 - val_loss: 0.3985\n",
      "Epoch 7/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 508ms/step - accuracy: 0.7109 - loss: 0.2692 - val_accuracy: 0.6566 - val_loss: 0.3499\n",
      "Epoch 8/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 499ms/step - accuracy: 0.7304 - loss: 0.2635 - val_accuracy: 0.6162 - val_loss: 0.3632\n",
      "Epoch 9/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 497ms/step - accuracy: 0.7535 - loss: 0.2490 - val_accuracy: 0.6263 - val_loss: 0.3506\n",
      "Epoch 10/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 498ms/step - accuracy: 0.7524 - loss: 0.2325 - val_accuracy: 0.5657 - val_loss: 0.3650\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 695ms/step - accuracy: 0.7157 - loss: 0.2434 - val_accuracy: 0.6970 - val_loss: 0.3353\n",
      "Epoch 2/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 678ms/step - accuracy: 0.8432 - loss: 0.1695 - val_accuracy: 0.6768 - val_loss: 0.3349\n",
      "Epoch 3/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 677ms/step - accuracy: 0.8900 - loss: 0.1402 - val_accuracy: 0.6667 - val_loss: 0.3748\n",
      "Epoch 4/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 676ms/step - accuracy: 0.9195 - loss: 0.1013 - val_accuracy: 0.7172 - val_loss: 0.3673\n",
      "Epoch 5/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 676ms/step - accuracy: 0.9409 - loss: 0.0796 - val_accuracy: 0.7172 - val_loss: 0.3991\n",
      "Epoch 6/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 682ms/step - accuracy: 0.9540 - loss: 0.0596 - val_accuracy: 0.7172 - val_loss: 0.3883\n",
      "Epoch 7/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 675ms/step - accuracy: 0.9682 - loss: 0.0448 - val_accuracy: 0.7273 - val_loss: 0.4217\n",
      "Epoch 8/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 678ms/step - accuracy: 0.9499 - loss: 0.0363 - val_accuracy: 0.7273 - val_loss: 0.4397\n",
      "Epoch 9/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 693ms/step - accuracy: 0.9639 - loss: 0.0289 - val_accuracy: 0.7172 - val_loss: 0.4534\n",
      "Epoch 10/10\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 694ms/step - accuracy: 0.9664 - loss: 0.0192 - val_accuracy: 0.7475 - val_loss: 0.5324\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze some layers of VGG19 and re-train\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[15:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Re-compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "history_fine = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_dataset\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
